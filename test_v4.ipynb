{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "import tqdm as tqdm\n",
    "import ast\n",
    "import gc\n",
    "import resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sql_loader import SqlLoader\n",
    "def limit_memory(maxsize):\n",
    "    # Set the maximum memory limit (in bytes)\n",
    "    soft, hard = resource.getrlimit(resource.RLIMIT_AS)\n",
    "    resource.setrlimit(resource.RLIMIT_AS, (maxsize, hard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_memory(12 * 1024 * 1024 * 1024)  # Limit to 8 GB \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''   \n",
    "\n",
    "input: json_folder\n",
    "- linearly iterate the fodler and run single file records.\n",
    "- exports single to another CSV folder (output)\n",
    "- \n",
    "output:\n",
    "\n",
    "\n",
    "'''\n",
    "def run_all_json():\n",
    "    # for json file in folder\n",
    "    #   run read_single_file json\n",
    "    # # export_single_file to single file csv to csv fodler\n",
    "    # return 0\n",
    "    pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "optimize memory: \n",
    "input: single linear loop of json logs.\n",
    "read / write datafame of single file\n",
    "output: single file linear iterate and write to csv.\n",
    "''' \n",
    "\n",
    "def read_single_file(json_file): # call at bottom\n",
    "    # read json (json_path)\n",
    "    df = pd.read_json(json_file) # read data frame from json file\n",
    "    # reformat: \n",
    "    result_column_df = df['data']['result']\n",
    "    all_values = []\n",
    "    for i in range(len(result_column_df)): \n",
    "        values = df['data']['result'][i]['values']  \n",
    "        all_values.extend(values)  \n",
    "    values_df = pd.Series(all_values)\n",
    "    values_df_rev = values_df.iloc[::-1]\n",
    "    return values_df_rev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''    \n",
    "get & fetch single file\n",
    "read single file.\n",
    "'''\n",
    "def collect_all_events(series):\n",
    "    '''  \n",
    "    Input: series variable of all read logs.\n",
    "    Output: a dataframe with events as data points in string format.\n",
    "    '''\n",
    "    start_indices = []\n",
    "    end_indices = []\n",
    "    for i, value in enumerate((series)):\n",
    "        if 'Start fetch event' in str(value):\n",
    "            start_indices.append(i)\n",
    "        elif '======END======' in str(value):\n",
    "            end_indices.append(i)  \n",
    "    log_segments = []\n",
    "    end_index = 0 \n",
    "\n",
    "    for start in (start_indices):\n",
    "        while end_index < len(end_indices) and end_indices[end_index] <= start:\n",
    "            end_index += 1\n",
    "        if end_index < len(end_indices):\n",
    "            end = end_indices[end_index]\n",
    "            segment = ' '.join(str(series[i]) for i in range(start + 1, end))\n",
    "            log_segments.append(segment)\n",
    "    event_data_df = pd.DataFrame(log_segments, columns=['Log String'])\n",
    "    event_data_df.index.name = 'Index'  \n",
    "    event_all_df = event_data_df['Log String']\n",
    "    return event_all_df  \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args(log):\n",
    "    '''  \n",
    "    input: log strings in JSON\n",
    "    output: arg data fields as dataFrame for single data point\n",
    "    '''\n",
    "    \n",
    "    if 'arg: {' in log:\n",
    "        arg_match = re.search(r'arg:\\s*(\\{.*?\\})', log)\n",
    "        if arg_match:\n",
    "            args_string = arg_match.group(1)\n",
    "            args_dict = ast.literal_eval(args_string) ##\n",
    "            df = pd.json_normalize(args_dict)\n",
    "            return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_args(event_all_df):\n",
    "        '''  \n",
    "        input: \n",
    "        output: dataFrame of all event args\n",
    "        '''\n",
    "        args_dfs = []\n",
    "        for log in ((event_all_df)):\n",
    "            log = str(log)\n",
    "            if isinstance(log, list):\n",
    "                log = log[0]\n",
    "            args_df = get_args(log)\n",
    "            args_dfs.append(args_df) # Null error\n",
    "        final_args_df = pd.concat(args_dfs, ignore_index=True)\n",
    "        \n",
    "        return(final_args_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_request_event_list(log):\n",
    "        '''  \n",
    "        '''\n",
    "        if 'Request get event list' in log:\n",
    "            request_match = re.search(r'Request get event list: (.*?)}', log) \n",
    "            if request_match:\n",
    "                request_string = request_match.group(1) + '}' \n",
    "                request_dict = ast.literal_eval(request_string)\n",
    "                df = pd.json_normalize(request_dict)\n",
    "                return df\n",
    "                \n",
    "            \n",
    "\n",
    "    \n",
    "def fetch_request_event_list(event_all_df):\n",
    "    ''' \n",
    "    \n",
    "    '''\n",
    "    request_event_list_dfs = []\n",
    "    for log in ((event_all_df)):\n",
    "        if isinstance(log, list):\n",
    "            log = log[0]  \n",
    "        req_df = get_request_event_list(log)\n",
    "        request_event_list_dfs.append(req_df)\n",
    "    final_request_event_list_df = pd.concat(request_event_list_dfs, ignore_index=True)\n",
    "    return(final_request_event_list_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get and fetch all, gather into single csv. (run_single_json)\n",
    "# loop the folder (run_json_fodler) ===> csv folder as ouut. \n",
    "\n",
    "\n",
    "def get_delay_alarm(log):\n",
    "    ''' \n",
    "    \n",
    "    '''\n",
    "    if 'Delay alarm event: ' in log: \n",
    "        delay_alarm_event_match = re.search(r'Delay alarm event: (.*?) s', log) \n",
    "        if delay_alarm_event_match:\n",
    "            delay_alarm_event_string = delay_alarm_event_match.group(1) \n",
    "            return pd.DataFrame({'DelayAlarm': [delay_alarm_event_string]})\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "def fetch_delay_alarm(event_all_df):\n",
    "    ''' \n",
    "    \n",
    "    '''\n",
    "    delay_alarm_dfs = []\n",
    "    for log in ((event_all_df)):\n",
    "        if isinstance(log, list):\n",
    "            log = log[0] \n",
    "        delay_alarm_event_df = get_delay_alarm(log)\n",
    "        delay_alarm_dfs.append(delay_alarm_event_df) \n",
    "    final_delay_alarm_df = pd.concat(delay_alarm_dfs, ignore_index=True)\n",
    "    return final_delay_alarm_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_time_query_event(log):\n",
    "    ''' \n",
    "    \n",
    "    '''\n",
    "    if 'Time query event: ' in log:\n",
    "        time_query_event_match = re.search(r'Time query event: (\\d+\\.\\d{3})', log) # search string & end string \n",
    "        if time_query_event_match:\n",
    "            time_query_event_string = time_query_event_match.group(1) \n",
    "            return pd.DataFrame({'TimeQueryEvent': [time_query_event_string]})\n",
    "        else:\n",
    "            return pd.DataFrame({'TimeQueryEvent': ['Not Found'],\n",
    "                    })\n",
    "    else:\n",
    "        return pd.DataFrame({'TimeQueryEvent': ['Not Found'],\n",
    "                    })\n",
    "\n",
    "def fetch_time_query_event(event_all_df):\n",
    "    ''' \n",
    "    \n",
    "    '''\n",
    "    time_query_event_dfs = []\n",
    "    for log in ((event_all_df)):\n",
    "        if isinstance(log, list):\n",
    "            log = log[0]  \n",
    "        time_query_event_df = get_time_query_event(log)\n",
    "        time_query_event_dfs.append(time_query_event_df) \n",
    "    final_time_query_event_df = pd.concat(time_query_event_dfs, ignore_index=True)\n",
    "    return(final_time_query_event_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_process_home_face(log):\n",
    "        '''\n",
    "         \n",
    "        '''\n",
    "        if 'Time query event: ' in log:\n",
    "            time_time_process_home_face_match = re.search(r'Time query event: (\\d+\\.\\d{3})', log) \n",
    "            if time_time_process_home_face_match:\n",
    "                time_time_process_home_face_string = time_time_process_home_face_match.group(1)\n",
    "                return pd.DataFrame({'TimeHomeFace': [time_time_process_home_face_string]})\n",
    "            else:\n",
    "                return pd.DataFrame({'TimeHomeFace': ['Not Found'],\n",
    "                                    })\n",
    "        else:\n",
    "            return pd.DataFrame({'TimeHomeFace': ['Not Found'],\n",
    "                        })\n",
    "        \n",
    "def fetch_time_process_home_face(event_all_df):\n",
    "    '''  \n",
    "    '''\n",
    "    time_process_home_face_dfs = []\n",
    "    for log in ((event_all_df)):\n",
    "        if isinstance(log, list):\n",
    "            log = log[0]  \n",
    "        time_process_home_face_df = get_time_process_home_face(log)\n",
    "        time_process_home_face_dfs.append(time_process_home_face_df) \n",
    "    final_time_process_home_face_df = pd.concat(time_process_home_face_dfs, ignore_index=True)\n",
    "    return final_time_process_home_face_df #df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_time_worker(log):\n",
    "        ''' \n",
    "        \n",
    "        \n",
    "        '''\n",
    "        if 'All time worker' in log:\n",
    "            all_time_worker_match = re.search(r\"All time worker: (.*?) s\" , log)\n",
    "            if all_time_worker_match:\n",
    "                all_time_worker_string = all_time_worker_match.group(1) \n",
    "                df = pd.DataFrame({'AllTimeWorker': [all_time_worker_string]})\n",
    "                return df\n",
    "            else:\n",
    "                pass\n",
    "                return pd.DataFrame({\n",
    "                        'AllTimeWorker': ['Not Found'],\n",
    "                        })    \n",
    "        else:\n",
    "            pass\n",
    "            return pd.DataFrame({\n",
    "                        'AllTimeWorker': ['Not Found'],\n",
    "                        })\n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "def fetch_all_time_worker(event_all_df):\n",
    "    ''' \n",
    "    \n",
    "    '''\n",
    "    all_time_worker_dfs = []\n",
    "    for log in ((event_all_df)):\n",
    "        if isinstance(log, list):\n",
    "            log = log[0]  \n",
    "\n",
    "        all_time_worker_event_df = get_all_time_worker(log)\n",
    "        all_time_worker_dfs.append(all_time_worker_event_df) \n",
    "    final_all_time_worker_df = pd.concat(all_time_worker_dfs, ignore_index=True)\n",
    "    return final_all_time_worker_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_alarm(log):\n",
    "        if 'Response get event list: ' in log: \n",
    "                response_match = re.search(r\" 'data': (.*?)'IsFinished': '1'}}\" , log)\n",
    "                if response_match:\n",
    "                    response_string = response_match.group(1) + '}' \n",
    "                    response_string = response_string.replace(\"'\", '\"')\n",
    "                    response_string = re.sub(r'\"recface\":\\s*\"({.*?})\"', r'\"recface\": \\1', response_string)\n",
    "                    response_string = re.sub(r',\\s*([\\]})])', r'\\1', response_string)\n",
    "                    response_dict = ast.literal_eval(response_string)\n",
    "                    df = pd.json_normalize(\n",
    "                        response_dict, \n",
    "                        record_path=  ['AlarmArray'],\n",
    "                        meta = ['SerialNumber'],\n",
    "                    )\n",
    "                    return df\n",
    "        \n",
    "\n",
    "def fetch_response_alarm(event_all_df):\n",
    "    response_event_dfs = []\n",
    "    for log in ((event_all_df)):\n",
    "        if isinstance(log, list):\n",
    "            log = log[0] \n",
    "        response_event_df = get_response_alarm(log)\n",
    "        response_event_dfs.append(response_event_df)\n",
    "    final_response_event_df = pd.concat(response_event_dfs, ignore_index=True)\n",
    "    return final_response_event_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_all(event_all_df):\n",
    "    ''' \n",
    "    '''\n",
    "    df_result = pd.concat(\n",
    "                    [\n",
    "                    fetch_args(event_all_df),\n",
    "                    fetch_request_event_list(event_all_df),\n",
    "                    fetch_time_query_event(event_all_df),\n",
    "                    fetch_time_process_home_face(event_all_df),\n",
    "                    fetch_all_time_worker(event_all_df)\n",
    "                    ],\n",
    "                    # ignore_index=True,\n",
    "                    axis=1)\n",
    "    response_alarm = fetch_response_alarm(event_all_df)\n",
    "    delay_alarm = fetch_delay_alarm(event_all_df)       \n",
    "    n = len(response_alarm)\n",
    "    response_alarm_subset = response_alarm.head(n)  \n",
    "    delay_alarm_subset = delay_alarm.head(n)    \n",
    "    \n",
    "    alarm_df = pd.concat(\n",
    "        [response_alarm_subset, delay_alarm_subset],\n",
    "        axis=1\n",
    "    )\n",
    "    alarm_df.drop('AlarmMsg', axis = 1)    \n",
    "    # alarm_df = pd.concat(\n",
    "    #                 [\n",
    "    #                 .fetch_response_alarm(),\n",
    "    #                 .fetch_delay_alarm()\n",
    "    #                 ],\n",
    "    #                 # ignore_index=True,\n",
    "    #                 axis=1)\n",
    "    \n",
    "    # postprocessing\n",
    "    \n",
    "    df_result = df_result.drop('token',axis = 1)\n",
    "    df_result = df_result.drop('time_millis',axis = 1)\n",
    "    df_result = df_result.drop('encrypted_str',axis = 1)\n",
    "    df_result = df_result.drop('startTime',axis = 1)\n",
    "    df_result = df_result.drop('endTime',axis = 1)\n",
    "    df_result = df_result.replace('Not Found', pd.NA).dropna() # not NA\n",
    "\n",
    "    return df_result , alarm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv_folder(file_index,\n",
    "                        event_df,\n",
    "                        alarm_df,\n",
    "                        event_csv_path,\n",
    "                        alarm_csv_path,\n",
    "                        chunk_size=10):  # Adjust chunk size as needed\n",
    "    \"\"\"\n",
    "    Write two DataFrames to specified CSV file paths in chunks.\n",
    "    Parameters:\n",
    "    - file_index (int): The index used to create the CSV file names.\n",
    "    - event_df (pd.DataFrame): DataFrame containing event data.\n",
    "    - alarm_df (pd.DataFrame): DataFrame containing alarm data.\n",
    "    - event_csv_path (str): Directory path to save the event CSV file.\n",
    "    - alarm_csv_path (str): Directory path to save the alarm CSV file.\n",
    "    - chunk_size (int): Number of rows to write at a time.\n",
    "    \"\"\"\n",
    "    event_file_name = f\"event_data_{file_index}.csv\"\n",
    "    alarm_file_name = f\"alarm_data_{file_index}.csv\"\n",
    "    \n",
    "    event_file_path = os.path.join(event_csv_path, event_file_name)\n",
    "    alarm_file_path = os.path.join(alarm_csv_path, alarm_file_name)\n",
    "    \n",
    "    # Write event_df to CSV in chunks\n",
    "    for i in range(0, len(event_df), chunk_size):\n",
    "        event_df.iloc[i:i + chunk_size].to_csv(event_file_path, mode='a', header=(i == 0), index=False)\n",
    "\n",
    "    # Write alarm_df to CSV in chunks\n",
    "    for i in range(0, len(alarm_df), chunk_size):\n",
    "        alarm_df.iloc[i:i + chunk_size].to_csv(alarm_file_path, mode='a', header=(i == 0), index=False)\n",
    "\n",
    "    # Free memory by deleting DataFrames\n",
    "    del event_df\n",
    "    del alarm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def write_all(json_folder_path):\n",
    "# iteratively read json files.\n",
    "    index = 0\n",
    "    for filename in tqdm.tqdm(os.listdir(json_folder_path), desc =\"Reading json files: \"):\n",
    "        if filename.endswith('.json'):  # Ensure the file is a JSON file \n",
    "            file_path = os.path.join(json_folder_path, filename)\n",
    "            json_df = read_single_file(file_path)\n",
    "            event_all_df = collect_all_events(json_df)\n",
    "            event_df , alarm_df = fetch_all(event_all_df)\n",
    "            \n",
    "            # run sql upload (packaged)\n",
    "             \n",
    "            # write_to_csv_folder(index,\n",
    "            # event_df,\n",
    "            # alarm_df, \n",
    "            # 'csv_output/event_csv/',\n",
    "            # 'csv_output/alarm_csv/')\n",
    "        index += 1\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading json files: 100%|██████████| 18/18 [09:52<00:00, 32.91s/it]\n"
     ]
    }
   ],
   "source": [
    "write_all('json_logs')\n",
    "# kernel crash at 15/18 \n",
    "# memory error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo: modify code log script & read from csv to SQL upload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
