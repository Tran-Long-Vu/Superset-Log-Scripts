{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "import tqdm as tqdm\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "temp_df = pd.DataFrame()\n",
    "\n",
    "path_to_json = './json_logs' \n",
    "\n",
    "\n",
    "json_pattern = os.path.join(path_to_json,'*.json')\n",
    "file_list = glob.glob(json_pattern)\n",
    "\n",
    "\n",
    "dfs = [] # an empty list to store the data frames\n",
    "for file in file_list:\n",
    "    # print(str(file))\n",
    "    data = pd.read_json(file) # read data frame from json file\n",
    "    dfs.append(data) # append the data frame to the list\n",
    "\n",
    "temp_df = pd.concat(dfs ) #  (pd.Series)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "result    [{'stream': {'app': 'worker-fetch-event', 'con...\n",
       "result    [{'stream': {'app': 'worker-fetch-event', 'con...\n",
       "result    [{'stream': {'app': 'worker-fetch-event', 'con...\n",
       "result    [{'stream': {'app': 'worker-fetch-event', 'con...\n",
       "result    [{'stream': {'app': 'worker-fetch-event', 'con...\n",
       "result    [{'stream': {'app': 'worker-fetch-event', 'con...\n",
       "result    [{'stream': {'app': 'worker-fetch-event', 'con...\n",
       "result    [{'stream': {'app': 'worker-fetch-event', 'con...\n",
       "result    [{'stream': {'app': 'worker-fetch-event', 'con...\n",
       "result    [{'stream': {'app': 'worker-fetch-event', 'con...\n",
       "result    [{'stream': {'app': 'worker-fetch-event', 'con...\n",
       "Name: data, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_column_df = temp_df['data']['result']\n",
    "result_column_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "print(str(len(result_column_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_78133/3165085121.py:5: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  values = temp_df['data']['result'][i][0]['values']  # Access the 'values' field\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize an empty list to gather all 'values'\n",
    "all_values = []\n",
    "# Loop through the indices of temp_df\n",
    "for i in range(len(result_column_df)):\n",
    "    values = temp_df['data']['result'][i][0]['values']  # Access the 'values' field\n",
    "    all_values.extend(values)  \n",
    "\n",
    "# Convert the list of values into a DataFrame\n",
    "values_df = pd.Series(all_values)\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "# print(values_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3508113\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0          [1723106449759523051, [2024-08-08 15:40:49,759...\n",
       "1          [1723106449749112639, [2024-08-08 15:40:49,748...\n",
       "2          [1723106449748911046, 2024-08-08 15:40:49.748 ...\n",
       "3          [1723106449748757857, 2024-08-08 15:40:49.748 ...\n",
       "4          [1723106449748393296, 2024-08-08 15:40:49.747 ...\n",
       "                                 ...                        \n",
       "3508108    [1723914006968846879, 2024-08-18 00:00:06.968 ...\n",
       "3508109    [1723914006968446207, 2024-08-18 00:00:06.968 ...\n",
       "3508110    [1723914006967799471, 2024-08-18 00:00:06.967 ...\n",
       "3508111    [1723914006967431632, 2024-08-18 00:00:06.966 ...\n",
       "3508112    [1723914006964275493, [2024-08-18 00:00:06,963...\n",
       "Length: 3508113, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(str(len(values_df)))\n",
    "\n",
    "\n",
    "# values_df.rename(columns={1:'Log String'}, inplace=True)\n",
    "\n",
    "\n",
    "# print(values_df.columns)\n",
    "values_df #.describe()\n",
    "# all data found.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_df_rev = values_df.iloc[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data point appears 316067 times.\n"
     ]
    }
   ],
   "source": [
    "# The specific data point to count\n",
    "search_string = '========================Start fetch event==========================='\n",
    "\n",
    "# Count occurrences of the search string\n",
    "count = sum('Start fetch event' in value[1] for value in values_df)\n",
    "# Display the coun t\n",
    "print(f\"The data point appears {count} times.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data point appears 7433 times.\n",
      "3508113\n"
     ]
    }
   ],
   "source": [
    "# The specific data point to count\n",
    "search_string = 'Delay alarm event:' # August 10th alarm count.\n",
    "\n",
    "# Count occurrences of the search string\n",
    "count = sum(search_string in value[1] for value in values_df_rev)\n",
    "# Display the coun t\n",
    "print(f\"The data point appears {count} times.\")\n",
    "\n",
    "print(str(len(values_df_rev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding indices: 100%|██████████| 3508113/3508113 [00:06<00:00, 556746.61it/s]\n",
      "Gathering log segments: 100%|██████████| 316067/316067 [00:05<00:00, 54353.23it/s]\n"
     ]
    }
   ],
   "source": [
    "def collect_all_events(series):\n",
    "    # Initialize lists to store indices\n",
    "    start_indices = []\n",
    "    end_indices = []\n",
    "    \n",
    "    # Iterate through the Series to find all start and end indices\n",
    "    for i, value in tqdm.tqdm(enumerate(series), total=len(series), desc=\"Finding indices\"):\n",
    "        if 'Start fetch event' in str(value):\n",
    "            start_indices.append(i)  # Capture the start index\n",
    "        elif '======END======' in str(value):\n",
    "            end_indices.append(i)  # Capture the end index\n",
    "\n",
    "    # Gather all segments between each pair of start and end indices\n",
    "    log_segments = []\n",
    "    end_index = 0  # Initialize end index pointer\n",
    "\n",
    "    for start in tqdm.tqdm(start_indices, desc=\"Gathering log segments\"):\n",
    "        # Move the end_index pointer to the first end index greater than start\n",
    "        while end_index < len(end_indices) and end_indices[end_index] <= start:\n",
    "            end_index += 1\n",
    "        \n",
    "        # If there is a valid end index, gather the log segment\n",
    "        if end_index < len(end_indices):\n",
    "            end = end_indices[end_index]\n",
    "            segment = ' '.join(str(series[i]) for i in range(start + 1, end))\n",
    "            log_segments.append(segment)\n",
    "\n",
    "    # Create a DataFrame with an index column and the combined log string\n",
    "    event_data_df = pd.DataFrame(log_segments, columns=['Log String'])\n",
    "    event_data_df.index.name = 'Index'  # Set the index name\n",
    "\n",
    "    return event_data_df\n",
    "\n",
    "event_data_df = collect_all_events(values_df_rev) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Log String</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Index</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['1723106449748911046', '2024-08-08 15:40:49.7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['1723106449691230012', '2024-08-08 15:40:49.6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['1723106449624719370', '[2024-08-08 15:40:49,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['1723106449572812804', '[2024-08-08 15:40:49,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['1723106449506242161', '[2024-08-08 15:40:49,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316062</th>\n",
       "      <td>['1723914016964469563', '[2024-08-18 00:00:16,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316063</th>\n",
       "      <td>['1723914007264492991', '[2024-08-18 00:00:07,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316064</th>\n",
       "      <td>['1723914007206810653', '[2024-08-18 00:00:07,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316065</th>\n",
       "      <td>['1723914007141020123', '[2024-08-18 00:00:07,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316066</th>\n",
       "      <td>['1723914007068239854', '2024-08-18 00:00:07.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>316067 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Log String\n",
       "Index                                                    \n",
       "0       ['1723106449748911046', '2024-08-08 15:40:49.7...\n",
       "1       ['1723106449691230012', '2024-08-08 15:40:49.6...\n",
       "2       ['1723106449624719370', '[2024-08-08 15:40:49,...\n",
       "3       ['1723106449572812804', '[2024-08-08 15:40:49,...\n",
       "4       ['1723106449506242161', '[2024-08-08 15:40:49,...\n",
       "...                                                   ...\n",
       "316062  ['1723914016964469563', '[2024-08-18 00:00:16,...\n",
       "316063  ['1723914007264492991', '[2024-08-18 00:00:07,...\n",
       "316064  ['1723914007206810653', '[2024-08-18 00:00:07,...\n",
       "316065  ['1723914007141020123', '[2024-08-18 00:00:07,...\n",
       "316066  ['1723914007068239854', '2024-08-18 00:00:07.0...\n",
       "\n",
       "[316067 rows x 1 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index\n",
      "0         ['1723106449748911046', '2024-08-08 15:40:49.7...\n",
      "1         ['1723106449691230012', '2024-08-08 15:40:49.6...\n",
      "2         ['1723106449624719370', '[2024-08-08 15:40:49,...\n",
      "3         ['1723106449572812804', '[2024-08-08 15:40:49,...\n",
      "4         ['1723106449506242161', '[2024-08-08 15:40:49,...\n",
      "                                ...                        \n",
      "316062    ['1723914016964469563', '[2024-08-18 00:00:16,...\n",
      "316063    ['1723914007264492991', '[2024-08-18 00:00:07,...\n",
      "316064    ['1723914007206810653', '[2024-08-18 00:00:07,...\n",
      "316065    ['1723914007141020123', '[2024-08-18 00:00:07,...\n",
      "316066    ['1723914007068239854', '2024-08-18 00:00:07.0...\n",
      "Name: Log String, Length: 316067, dtype: object\n"
     ]
    }
   ],
   "source": [
    "event_data_log_string = event_data_df['Log String']\n",
    "\n",
    "print(event_data_log_string) # 34650 event, in a log string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract args from log string\n",
    "def get_args(log):\n",
    "    if 'arg: {' in log:\n",
    "        # Use a regular expression to find the arguments\n",
    "        arg_match = re.search(r'arg:\\s*(\\{.*?\\})', log)\n",
    "        if arg_match:\n",
    "            # Extract the matched group\n",
    "            args_string = arg_match.group(1)\n",
    "            \n",
    "            # Convert the string representation of the dictionary to an actual dictionary\n",
    "            args_dict = ast.literal_eval(args_string)\n",
    "            # Normalize the dictionary into a DataFrame\n",
    "            df = pd.json_normalize(args_dict)\n",
    "            return df  # Return the DataFrame\n",
    "        else:\n",
    "            # Return a DataFrame with '0' values as strings if parsing fails\n",
    "           return pd.DataFrame({'sn': ['Not Found'],\n",
    "                                'user_id': ['Not Found'],\n",
    "                                'token': ['Not Found'],\n",
    "                                'time_millis': ['Not Found'], \n",
    "                                'encrypted_str': ['Not Found'], \n",
    "                                'time_query_latest': ['Not Found'], \n",
    "                                'datetime': ['Not Found']})\n",
    "    else:\n",
    "        # Return a DataFrame with '0' values as strings if 'arg: {' is not found\n",
    "        return pd.DataFrame({'sn': ['Not Found'],\n",
    "                            'user_id': ['Not Found'],\n",
    "                            'token': ['Not Found'], \n",
    "                            'time_millis': ['Not Found'],\n",
    "                            'encrypted_str': ['Not Found'],\n",
    "                            'time_query_latest': ['Not Found'], \n",
    "                            'datetime': ['Not Found']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding args:   0%|          | 0/316067 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (<unknown>, line 1)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3577\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[55], line 22\u001b[0m\n    fetch_args()\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[55], line 11\u001b[0m in \u001b[1;35mfetch_args\u001b[0m\n    args_df = get_args(log)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[42], line 11\u001b[0m in \u001b[1;35mget_args\u001b[0m\n    args_dict = ast.literal_eval(args_string)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m/usr/lib/python3.10/ast.py:64\u001b[0m in \u001b[1;35mliteral_eval\u001b[0m\n    node_or_string = parse(node_or_string.lstrip(\" \\t\"), mode='eval')\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m/usr/lib/python3.10/ast.py:50\u001b[0;36m in \u001b[0;35mparse\u001b[0;36m\n\u001b[0;31m    return compile(source, filename, mode, flags,\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m<unknown>:1\u001b[0;36m\u001b[0m\n\u001b[0;31m    {\\'sn\\': \\'ba39b3962a4bfe81\\', \\'user_id\\': \\'5afa5178-c4a6-4eda-af3d-4f816135cea4\\', \\'token\\': \\'ZTMzOTRmOWRkY3xiYTM5YjM5NjJhNGJmZTgxfDVhZmE1MTc4LWM0YTYtNGVkYS1h2ZjNkLTRmODE2MTM1Y2VhNHwxNzIzMTgyNDAyMDQzfDF8MTcyLjE2LjMwLjV8MQ%3D%3D.b3ea55ceb9e522d1ce772b28933e03e1\\', \\'time_millis\\': \\'00002131723096002012\\', \\'encrypted_str\\': \\'0dac76032657aaca2216eda2d6e4e0d7\\', \\'time_query_latest\\': \\'2024-08-08 15:40:30\\', \\'datetime\\': \\'2024-08-08 15:40:39\\'}\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "# 70s query time\n",
    "def fetch_args():\n",
    "    # List to store DataFrames\n",
    "    args_dfs = []\n",
    "\n",
    "    # Iterate over each log string and extract args\n",
    "    for log in tqdm.tqdm((event_data_log_string), total=len(event_data_log_string), desc=\"Finding args\"):\n",
    "        ##### expect data error after enumerate.\n",
    "        log = str(log)\n",
    "        #print\n",
    "        args_df = get_args(log)\n",
    "        args_dfs.append(args_df)\n",
    "\n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    final_args_df = pd.concat(args_dfs, ignore_index=True)\n",
    "\n",
    "    # Display the final DataFrame\n",
    "    print(final_args_df)\n",
    "    return(final_args_df)\n",
    "\n",
    "\n",
    "fetch_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_request_event_list(log):\n",
    "    if 'Request get event list' in log: # if string in log\n",
    "        \n",
    "        # Use a regular expression to find the arguments\n",
    "        request_match = re.search(r'Request get event list: (.*?)}', log) # search string & end string \n",
    "        if request_match:\n",
    "            # Extract the matched group\n",
    "            request_string = request_match.group(1) + '}' # closing  signs\n",
    "                # Convert the string representation of the dictionary to an actual dictionary\n",
    "            request_dict = ast.literal_eval(request_string)\n",
    "            # Normalize the dictionary into a DataFrame\n",
    "            df = pd.json_normalize(request_dict)\n",
    "            return df  # Return the DataFrame\n",
    "            # except Exception as e:\n",
    "        else:\n",
    "            return pd.DataFrame({'sn': ['Not Found'],\n",
    "                                'startTime': ['Not Found'],\n",
    "                                'endTime': ['Not Found'],\n",
    "                                })\n",
    "    else:\n",
    "        # Return a DataFrame with '0' values as strings if 'arg: {' is not found\n",
    "        return pd.DataFrame({'sn': ['Not Found'],\n",
    "                    'startTime': ['Not Found'],\n",
    "                    'endTime': ['Not Found'],\n",
    "                    })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      sn            startTime              endTime\n",
      "0       ba39b3962a4bfe81  2024-08-08 15:40:30  2024-08-08 15:40:39\n",
      "1              Not Found            Not Found            Not Found\n",
      "2              Not Found            Not Found            Not Found\n",
      "3              Not Found            Not Found            Not Found\n",
      "4              Not Found            Not Found            Not Found\n",
      "...                  ...                  ...                  ...\n",
      "316062         Not Found            Not Found            Not Found\n",
      "316063         Not Found            Not Found            Not Found\n",
      "316064         Not Found            Not Found            Not Found\n",
      "316065  fc5f9a3b7707d489  2024-08-17 23:59:47  2024-08-17 23:59:56\n",
      "316066  fb1f6e5c62b71703  2024-08-17 23:59:47  2024-08-17 23:59:56\n",
      "\n",
      "[316067 rows x 3 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sn</th>\n",
       "      <th>startTime</th>\n",
       "      <th>endTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ba39b3962a4bfe81</td>\n",
       "      <td>2024-08-08 15:40:30</td>\n",
       "      <td>2024-08-08 15:40:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not Found</td>\n",
       "      <td>Not Found</td>\n",
       "      <td>Not Found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not Found</td>\n",
       "      <td>Not Found</td>\n",
       "      <td>Not Found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Not Found</td>\n",
       "      <td>Not Found</td>\n",
       "      <td>Not Found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not Found</td>\n",
       "      <td>Not Found</td>\n",
       "      <td>Not Found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316062</th>\n",
       "      <td>Not Found</td>\n",
       "      <td>Not Found</td>\n",
       "      <td>Not Found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316063</th>\n",
       "      <td>Not Found</td>\n",
       "      <td>Not Found</td>\n",
       "      <td>Not Found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316064</th>\n",
       "      <td>Not Found</td>\n",
       "      <td>Not Found</td>\n",
       "      <td>Not Found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316065</th>\n",
       "      <td>fc5f9a3b7707d489</td>\n",
       "      <td>2024-08-17 23:59:47</td>\n",
       "      <td>2024-08-17 23:59:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316066</th>\n",
       "      <td>fb1f6e5c62b71703</td>\n",
       "      <td>2024-08-17 23:59:47</td>\n",
       "      <td>2024-08-17 23:59:56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>316067 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      sn            startTime              endTime\n",
       "0       ba39b3962a4bfe81  2024-08-08 15:40:30  2024-08-08 15:40:39\n",
       "1              Not Found            Not Found            Not Found\n",
       "2              Not Found            Not Found            Not Found\n",
       "3              Not Found            Not Found            Not Found\n",
       "4              Not Found            Not Found            Not Found\n",
       "...                  ...                  ...                  ...\n",
       "316062         Not Found            Not Found            Not Found\n",
       "316063         Not Found            Not Found            Not Found\n",
       "316064         Not Found            Not Found            Not Found\n",
       "316065  fc5f9a3b7707d489  2024-08-17 23:59:47  2024-08-17 23:59:56\n",
       "316066  fb1f6e5c62b71703  2024-08-17 23:59:47  2024-08-17 23:59:56\n",
       "\n",
       "[316067 rows x 3 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "fetch req event\n",
    "\n",
    "'''\n",
    "\n",
    "def fetch_request_event():\n",
    "\n",
    "    # List to store DataFrames\n",
    "    request_event_list_dfs = []\n",
    "    # Iterate over each log string and extract args\n",
    "    for log in event_data_log_string:\n",
    "        # Convert log string from list format to string if necessary\n",
    "        if isinstance(log, list):\n",
    "            log = log[0]  \n",
    "\n",
    "        req_df = get_request_event_list(log)\n",
    "        request_event_list_dfs.append(req_df) # append to extend. ()\n",
    "\n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    final_request_event_list_df = pd.concat(request_event_list_dfs, ignore_index=True)\n",
    "\n",
    "    # Display the final DataFrame\n",
    "    print(final_request_event_list_df)\n",
    "    return(final_request_event_list_df)\n",
    "fetch_request_event()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''   \n",
    "get time query event\n",
    "'''\n",
    "\n",
    "\n",
    "def get_time_query_event(log):\n",
    "    if 'Time query event: ' in log: # if string in log\n",
    "        \n",
    "        # Use a regular expression to find the arguments\n",
    "        time_query_event_match = re.search(r'Time query event: (\\d+\\.\\d{3})', log) # search string & end string \n",
    "        if time_query_event_match:\n",
    "            # Extract the matched group\n",
    "            time_query_event_string = time_query_event_match.group(1) # closing  signs\n",
    "                # Convert the string representation of the dictionary to an actual dictionary\n",
    "            # since its a raw string, cannot be turned to JSON dict.\n",
    "            return pd.DataFrame({'TimeQueryEvent': [time_query_event_string]})\n",
    "        else:\n",
    "            return pd.DataFrame({'TimeQueryEvent': ['Not Found'],\n",
    "                                })\n",
    "    else:\n",
    "        # Return a DataFrame with '0' values as strings if 'arg: {' is not found\n",
    "        return pd.DataFrame({'TimeQueryEvent': ['Not Found'],\n",
    "                    })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       TimeQueryEvent\n",
      "0               0.038\n",
      "1               0.036\n",
      "2               0.033\n",
      "3               0.045\n",
      "4           Not Found\n",
      "...               ...\n",
      "316062      Not Found\n",
      "316063          0.036\n",
      "316064          0.037\n",
      "316065          0.038\n",
      "316066          0.096\n",
      "\n",
      "[316067 rows x 1 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TimeQueryEvent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not Found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316062</th>\n",
       "      <td>Not Found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316063</th>\n",
       "      <td>0.036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316064</th>\n",
       "      <td>0.037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316065</th>\n",
       "      <td>0.038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316066</th>\n",
       "      <td>0.096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>316067 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       TimeQueryEvent\n",
       "0               0.038\n",
       "1               0.036\n",
       "2               0.033\n",
       "3               0.045\n",
       "4           Not Found\n",
       "...               ...\n",
       "316062      Not Found\n",
       "316063          0.036\n",
       "316064          0.037\n",
       "316065          0.038\n",
       "316066          0.096\n",
       "\n",
       "[316067 rows x 1 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Fetch time query event df\n",
    "'''\n",
    "\n",
    "def fetch_time_query_event():\n",
    "        \n",
    "    # List to store DataFrames\n",
    "    time_query_event_dfs = []\n",
    "    # Iterate over each log string and extract args\n",
    "    for log in event_data_log_string:\n",
    "        # Convert log string from list format to string if necessary\n",
    "        if isinstance(log, list):\n",
    "            log = log[0]  # Assuming the first element is the log string\n",
    "\n",
    "        time_query_event_df = get_time_query_event(log)\n",
    "        time_query_event_dfs.append(time_query_event_df) # string error. \n",
    "\n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    final_time_query_event_df = pd.concat(time_query_event_dfs, ignore_index=True)\n",
    "\n",
    "    # Display the final DataFrame\n",
    "    print(final_time_query_event_df)\n",
    "    return(final_time_query_event_df)\n",
    "\n",
    "fetch_time_query_event()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "function extract all time worker\n",
    "\n",
    "'''\n",
    "\n",
    "def get_all_time_worker(log):\n",
    "    if 'All time worker' in log: # if string in log\n",
    "        # Use a regular expression to find the arguments\n",
    "        # regex search conflict.\n",
    "        all_time_worker_match = re.search(r\"All time worker: (.*?) s\" , log)\n",
    "        if all_time_worker_match:\n",
    "            # Extract the JSON-like string\n",
    "            all_time_worker_string = all_time_worker_match.group(1)  # Append closing braces\n",
    "            # string type, do not need dict.\n",
    "            df = pd.DataFrame({'AllTimeWorker': [all_time_worker_string]})\n",
    "            return df\n",
    "        else:  # not match   \n",
    "            return pd.DataFrame({\n",
    "                    'AllTimeWorker': ['Not Found'],\n",
    "                    })    \n",
    "    else:\n",
    "        # Return a DataFrame with '0' values as strings if 'arg: {' is not found\n",
    "        return pd.DataFrame({\n",
    "                    'AllTimeWorker': ['Not Found'],\n",
    "                    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               AllTimeWorker\n",
      "0        0.04169297218322754\n",
      "1        0.06423783302307129\n",
      "2       0.037348031997680664\n",
      "3        0.04944801330566406\n",
      "4        0.04588580131530762\n",
      "...                      ...\n",
      "316062   0.03814101219177246\n",
      "316063   0.04008007049560547\n",
      "316064   0.04249310493469238\n",
      "316065   0.04337191581726074\n",
      "316066   0.10049295425415039\n",
      "\n",
      "[316067 rows x 1 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AllTimeWorker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.04169297218322754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.06423783302307129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.037348031997680664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.04944801330566406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.04588580131530762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316062</th>\n",
       "      <td>0.03814101219177246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316063</th>\n",
       "      <td>0.04008007049560547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316064</th>\n",
       "      <td>0.04249310493469238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316065</th>\n",
       "      <td>0.04337191581726074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316066</th>\n",
       "      <td>0.10049295425415039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>316067 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               AllTimeWorker\n",
       "0        0.04169297218322754\n",
       "1        0.06423783302307129\n",
       "2       0.037348031997680664\n",
       "3        0.04944801330566406\n",
       "4        0.04588580131530762\n",
       "...                      ...\n",
       "316062   0.03814101219177246\n",
       "316063   0.04008007049560547\n",
       "316064   0.04249310493469238\n",
       "316065   0.04337191581726074\n",
       "316066   0.10049295425415039\n",
       "\n",
       "[316067 rows x 1 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fetch_all_time_worker():\n",
    "\n",
    "    # List to store DataFrames\n",
    "    all_time_worker_dfs = []\n",
    "    # Iterate over each log string and extract args\n",
    "    for log in event_data_log_string:\n",
    "        # Convert log string from list format to string if necessary\n",
    "        if isinstance(log, list):\n",
    "            log = log[0]  # Assuming the first element is the log string\n",
    "\n",
    "        all_time_worker_event_df = get_all_time_worker(log)\n",
    "        all_time_worker_dfs.append(all_time_worker_event_df) # string error. \n",
    "\n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    final_all_time_worker_df = pd.concat(all_time_worker_dfs, ignore_index=True)\n",
    "\n",
    "    # Display the final DataFrame\n",
    "    print(final_all_time_worker_df)\n",
    "    return final_all_time_worker_df\n",
    "    \n",
    "fetch_all_time_worker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''  \n",
    "function for delay alarm event (limited df.)\n",
    "\n",
    "'''\n",
    "\n",
    "def get_delay_alarm_event(log):\n",
    "    if 'Delay alarm event: ' in log: # if string in log\n",
    "        \n",
    "        # Use a regular expression to find the arguments\n",
    "        delay_alarm_event_match = re.search(r' Delay alarm event: (.*?) s', log) # search string & end string \n",
    "        if delay_alarm_event_match:\n",
    "            # Extract the matched group\n",
    "            delay_alarm_event_string = delay_alarm_event_match.group(1) # closing  signs\n",
    "                # Convert the string representation of the dictionary to an actual dictionary\n",
    "            # since its a raw string, cannot be turned to JSON dict.\n",
    "            # print(delay_alarm_event_string)\n",
    "            return pd.DataFrame({'DelayAlarm': [delay_alarm_event_string]})\n",
    "        else:\n",
    "            pass\n",
    "            # return pd.DataFrame({'DelayAlarm': ['Not Found'],\n",
    "            #                    })\n",
    "    else:\n",
    "        pass\n",
    "        # Return a DataFrame with '0' values as strings if 'arg: {' is not found\n",
    "        # return pd.DataFrame({'DelayAlarm': ['Not Found'],\n",
    "        #            })\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     DelayAlarm\n",
      "0            12\n",
      "1            14\n",
      "2            18\n",
      "3            16\n",
      "4            19\n",
      "...         ...\n",
      "4949         18\n",
      "4950         13\n",
      "4951         20\n",
      "4952         11\n",
      "4953         13\n",
      "\n",
      "[4954 rows x 1 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DelayAlarm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4949</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4950</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4951</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4952</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4953</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4954 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     DelayAlarm\n",
       "0            12\n",
       "1            14\n",
       "2            18\n",
       "3            16\n",
       "4            19\n",
       "...         ...\n",
       "4949         18\n",
       "4950         13\n",
       "4951         20\n",
       "4952         11\n",
       "4953         13\n",
       "\n",
       "[4954 rows x 1 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fetch_delay_alarm():\n",
    "        # List to store DataFrames\n",
    "    delay_alarm_dfs = []\n",
    "    # Iterate over each log string and extract args\n",
    "    for log in event_data_log_string:\n",
    "        # Convert log string from list format to string if necessary\n",
    "        if isinstance(log, list):\n",
    "            log = log[0]  # Assuming the first element is the log string\n",
    "\n",
    "        delay_alarm_event_df = get_delay_alarm_event(log)\n",
    "        delay_alarm_dfs.append(delay_alarm_event_df) # string error. \n",
    "\n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    final_delay_alarm_df = pd.concat(delay_alarm_dfs, ignore_index=True)\n",
    "\n",
    "    # Display the final DataFrame\n",
    "    print(final_delay_alarm_df)\n",
    "    return final_delay_alarm_df\n",
    "    \n",
    "fetch_delay_alarm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''   \n",
    "time_process_home_face\n",
    "\n",
    "'''\n",
    "\n",
    "def get_time_process_home_face(log):\n",
    "    if 'Time query event: ' in log: # if string in log\n",
    "        \n",
    "        # Use a regular expression to find the arguments\n",
    "        time_time_process_home_face_match = re.search(r'Time query event: (\\d+\\.\\d{3})', log) # search string & end string \n",
    "        if time_time_process_home_face_match:\n",
    "            # Extract the matched group\n",
    "            time_time_process_home_face_string = time_time_process_home_face_match.group(1) # closing  signs\n",
    "                # Convert the string representation of the dictionary to an actual dictionary\n",
    "            # since its a raw string, cannot be turned to JSON dict.\n",
    "            return pd.DataFrame({'TimeHomeFace': [time_time_process_home_face_string]})\n",
    "        else:\n",
    "            return pd.DataFrame({'TimeHomeFace': ['Not Found'],\n",
    "                                })\n",
    "    else:\n",
    "        # Return a DataFrame with '0' values as strings if 'arg: {' is not found\n",
    "        return pd.DataFrame({'TimeHomeFace': ['Not Found'],\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       TimeHomeFace\n",
      "0             0.038\n",
      "1             0.036\n",
      "2             0.033\n",
      "3             0.045\n",
      "4         Not Found\n",
      "...             ...\n",
      "316062    Not Found\n",
      "316063        0.036\n",
      "316064        0.037\n",
      "316065        0.038\n",
      "316066        0.096\n",
      "\n",
      "[316067 rows x 1 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TimeHomeFace</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not Found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316062</th>\n",
       "      <td>Not Found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316063</th>\n",
       "      <td>0.036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316064</th>\n",
       "      <td>0.037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316065</th>\n",
       "      <td>0.038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316066</th>\n",
       "      <td>0.096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>316067 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       TimeHomeFace\n",
       "0             0.038\n",
       "1             0.036\n",
       "2             0.033\n",
       "3             0.045\n",
       "4         Not Found\n",
       "...             ...\n",
       "316062    Not Found\n",
       "316063        0.036\n",
       "316064        0.037\n",
       "316065        0.038\n",
       "316066        0.096\n",
       "\n",
       "[316067 rows x 1 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fetch_time_process_home_face():\n",
    "        # List to store DataFrames\n",
    "    time_process_home_face_dfs = []\n",
    "    # Iterate over each log string and extract args\n",
    "    for log in event_data_log_string:\n",
    "        # Convert log string from list format to string if necessary\n",
    "        if isinstance(log, list):\n",
    "            log = log[0]  # Assuming the first element is the log string\n",
    "\n",
    "        time_process_home_face_df = get_time_process_home_face(log)\n",
    "        time_process_home_face_dfs.append(time_process_home_face_df) # string error. \n",
    "\n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    final_time_process_home_face_df = pd.concat(time_process_home_face_dfs, ignore_index=True)\n",
    "\n",
    "    # Display the final DataFrame\n",
    "    print(final_time_process_home_face_df)\n",
    "    return(final_time_process_home_face_df)\n",
    "    \n",
    "fetch_time_process_home_face()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_result \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([ \u001b[43mfetch_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m      2\u001b[0m                         fetch_request_event(),\n\u001b[1;32m      3\u001b[0m                         fetch_delay_alarm(),\n\u001b[1;32m      4\u001b[0m                         fetch_time_query_event(),\n\u001b[1;32m      5\u001b[0m                         fetch_time_process_home_face(),\n\u001b[1;32m      6\u001b[0m                         fetch_all_time_worker(),\n\u001b[1;32m      7\u001b[0m                         \u001b[38;5;66;03m# final_response_event_df\u001b[39;00m\n\u001b[1;32m      8\u001b[0m                         ],\n\u001b[1;32m      9\u001b[0m                         \u001b[38;5;66;03m# ignore_index=True,\u001b[39;00m\n\u001b[1;32m     10\u001b[0m                         axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     11\u001b[0m df_result\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# linear runtime: 3mins\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[43], line 11\u001b[0m, in \u001b[0;36mfetch_args\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     log \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(log)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m#print\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     args_df \u001b[38;5;241m=\u001b[39m \u001b[43mget_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     args_dfs\u001b[38;5;241m.\u001b[39mappend(args_df)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Concatenate all DataFrames into a single DataFrame\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[42], line 13\u001b[0m, in \u001b[0;36mget_args\u001b[0;34m(log)\u001b[0m\n\u001b[1;32m     11\u001b[0m     args_dict \u001b[38;5;241m=\u001b[39m ast\u001b[38;5;241m.\u001b[39mliteral_eval(args_string)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Normalize the dictionary into a DataFrame\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson_normalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df  \u001b[38;5;66;03m# Return the DataFrame\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Return a DataFrame with '0' values as strings if parsing fails\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/json/_normalize.py:457\u001b[0m, in \u001b[0;36mjson_normalize\u001b[0;34m(data, record_path, meta, meta_prefix, record_prefix, errors, sep, max_level)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# check to see if a simple recursive function is possible to\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;66;03m# improve performance (see #15621) but only for cases such\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;66;03m# as pd.Dataframe(data) or pd.Dataframe(data, sep)\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    451\u001b[0m     record_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    452\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m meta \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m max_level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    456\u001b[0m ):\n\u001b[0;32m--> 457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_simple_json_normalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m record_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m([\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m y\u001b[38;5;241m.\u001b[39mvalues()] \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m data):\n\u001b[1;32m    461\u001b[0m         \u001b[38;5;66;03m# naive normalization, this is idempotent for flat records\u001b[39;00m\n\u001b[1;32m    462\u001b[0m         \u001b[38;5;66;03m# and potentially will inflate the data considerably for\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[38;5;66;03m# TODO: handle record value which are lists, at least error\u001b[39;00m\n\u001b[1;32m    467\u001b[0m         \u001b[38;5;66;03m#       reasonably\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:851\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    849\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    850\u001b[0m         columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[0;32m--> 851\u001b[0m     arrays, columns, index \u001b[38;5;241m=\u001b[39m \u001b[43mnested_data_to_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;49;00m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;49;00m\n\u001b[1;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    857\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    859\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[1;32m    860\u001b[0m         arrays,\n\u001b[1;32m    861\u001b[0m         columns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    864\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[1;32m    865\u001b[0m     )\n\u001b[1;32m    866\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/internals/construction.py:520\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[0;34m(data, columns, index, dtype)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_named_tuple(data[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    518\u001b[0m     columns \u001b[38;5;241m=\u001b[39m ensure_index(data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fields)\n\u001b[0;32m--> 520\u001b[0m arrays, columns \u001b[38;5;241m=\u001b[39m \u001b[43mto_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/internals/construction.py:845\u001b[0m, in \u001b[0;36mto_arrays\u001b[0;34m(data, columns, dtype)\u001b[0m\n\u001b[1;32m    842\u001b[0m     data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mtuple\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m    843\u001b[0m     arr \u001b[38;5;241m=\u001b[39m _list_to_arrays(data)\n\u001b[0;32m--> 845\u001b[0m content, columns \u001b[38;5;241m=\u001b[39m \u001b[43m_finalize_columns_and_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m content, columns\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/internals/construction.py:945\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[0;34m(content, columns, dtype)\u001b[0m\n\u001b[1;32m    942\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(contents) \u001b[38;5;129;01mand\u001b[39;00m contents[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mobject_:\n\u001b[0;32m--> 945\u001b[0m     contents \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_object_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m contents, columns\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/internals/construction.py:1070\u001b[0m, in \u001b[0;36mconvert_object_array\u001b[0;34m(content, dtype, dtype_backend, coerce_float)\u001b[0m\n\u001b[1;32m   1066\u001b[0m             arr \u001b[38;5;241m=\u001b[39m maybe_cast_to_datetime(arr, dtype)\n\u001b[1;32m   1068\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\n\u001b[0;32m-> 1070\u001b[0m arrays \u001b[38;5;241m=\u001b[39m [convert(arr) \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m content]\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arrays\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/internals/construction.py:1070\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1066\u001b[0m             arr \u001b[38;5;241m=\u001b[39m maybe_cast_to_datetime(arr, dtype)\n\u001b[1;32m   1068\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\n\u001b[0;32m-> 1070\u001b[0m arrays \u001b[38;5;241m=\u001b[39m [\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m content]\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arrays\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/internals/construction.py:1030\u001b[0m, in \u001b[0;36mconvert_object_array.<locals>.convert\u001b[0;34m(arr)\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(arr):\n\u001b[1;32m   1029\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1030\u001b[0m         arr \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaybe_convert_objects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m            \u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtry_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoerce_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert_to_nullable_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnumpy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1035\u001b[0m         \u001b[38;5;66;03m# Notes on cases that get here 2023-02-15\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m         \u001b[38;5;66;03m# 1) we DO get here when arr is all Timestamps and dtype=None\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m         \u001b[38;5;66;03m# 2) disabling this doesn't break the world, so this must be\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m         \u001b[38;5;66;03m#    getting caught at a higher level\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m         \u001b[38;5;66;03m# 3) passing convert_non_numeric to maybe_convert_objects get this right\u001b[39;00m\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;66;03m# 4) convert_non_numeric?\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32mlib.pyx:2543\u001b[0m, in \u001b[0;36mpandas._libs.lib.maybe_convert_objects\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/_core/numeric.py:303\u001b[0m, in \u001b[0;36mfull\u001b[0;34m(shape, fill_value, dtype, order, device, like)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_full_dispatcher\u001b[39m(\n\u001b[1;32m    298\u001b[0m     shape, fill_value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, like\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    299\u001b[0m ):\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m(like,)\n\u001b[0;32m--> 303\u001b[0m \u001b[38;5;129m@set_array_function_like_doc\u001b[39m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;129m@set_module\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfull\u001b[39m(shape, fill_value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, like\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    306\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;124;03m    Return a new array of given shape and type, filled with `fill_value`.\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    355\u001b[0m \n\u001b[1;32m    356\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m like \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_result = pd.concat([ fetch_args(),\n",
    "                        fetch_request_event(),\n",
    "                        fetch_delay_alarm(),\n",
    "                        fetch_time_query_event(),\n",
    "                        fetch_time_process_home_face(),\n",
    "                        fetch_all_time_worker(),\n",
    "                        # final_response_event_df\n",
    "                        ],\n",
    "                        # ignore_index=True,\n",
    "                        axis=1)\n",
    "df_result\n",
    "# linear runtime: 3mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'csv_output'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./csv_output/event_logs_v3.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/util/_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    332\u001b[0m     )\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/generic.py:3967\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3956\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[1;32m   3958\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[1;32m   3959\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[1;32m   3960\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3964\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[1;32m   3965\u001b[0m )\n\u001b[0;32m-> 3967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3970\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3972\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3984\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/formats/format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[1;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[1;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[1;32m   1013\u001b[0m )\n\u001b[0;32m-> 1014\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/formats/csvs.py:251\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[1;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[1;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[1;32m    268\u001b[0m     )\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/common.py:749\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[0;32m--> 749\u001b[0m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/common.py:616\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    614\u001b[0m parent \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[0;32m--> 616\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'csv_output'"
     ]
    }
   ],
   "source": [
    "df_result.to_csv('./csv_output/event_logs_v3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_event(log):\n",
    "    \n",
    "    if 'Response get event list: ' in log: # if string in log\n",
    "        # Use a regular expression to find the arguments\n",
    "        # regex search conflict.\n",
    "        response_match = re.search(r\" 'data': (.*?)'IsFinished': '1'}}\" , log)\n",
    "        if response_match:\n",
    "            # Extract the JSON-like string\n",
    "            response_string = response_match.group(1) + '}'  # Append closing braces\n",
    "            \n",
    "            # Replace single quotes with double quotes\n",
    "            response_string = response_string.replace(\"'\", '\"')\n",
    "            \n",
    "            # Fix the nested JSON string in 'LabelInfo'\n",
    "            response_string = re.sub(r'\"recface\":\\s*\"({.*?})\"', r'\"recface\": \\1', response_string)\n",
    "            \n",
    "            # Remove trailing commas before closing braces\n",
    "            response_string = re.sub(r',\\s*([\\]})])', r'\\1', response_string)\n",
    "            response_dict = ast.literal_eval(response_string)\n",
    "            \n",
    "            \n",
    "            # hard code return the dataframe. \n",
    "            # Normalize the dictionary into a DataFrame\n",
    "            df = pd.json_normalize(\n",
    "                response_dict, \n",
    "                record_path=  ['AlarmArray'], # Path to the nested list\n",
    "                            \n",
    "                # meta=[\n",
    "                #     [ 'SerialNumber'],  # Include SerialNumber\n",
    "                #     [ 'AlarmTotal'],     # Include AlarmTotal  \n",
    "                #     # Include all fields\n",
    "                #     ['AlarmArray','AlarmEvent'],\n",
    "                #     ['AlarmArray','AlarmId'],\n",
    "                #     ['AlarmArray','AlarmMsg'],\n",
    "                #     ['AlarmArray','AlarmTime'],\n",
    "                #     ['AlarmArray','Channel'],\n",
    "\n",
    "                \n",
    "            )\n",
    "            return df\n",
    "        \n",
    "        else:  # not match   \n",
    "            pass\n",
    "            # return pd.DataFrame({ # 11 columns\n",
    "            #         'SerialNumber': ['Not Found'],\n",
    "            #         'AlarmTotal': ['Not Found'],\n",
    "                    \n",
    "            #         'AlarmEvent': ['Not Found'],\n",
    "            #         'AlarmId': ['Not Found'],\n",
    "            #         'AlarmMsg': ['Not Found'],\n",
    "            #         'AlarmTime': ['Not Found'],\n",
    "            #         'Channel': ['Not Found'],\n",
    "                    \n",
    "            #         'PicInfo.ObjName': ['Not Found'],\n",
    "            #         'PicInfo.ObjSize': ['Not Found'],\n",
    "            #         'PicInfo.StorageBucket': ['Not Found'],\n",
    "            #         'VideoInfo.VideoLength': ['Not Found'],\n",
    "            #         })    \n",
    "    else:\n",
    "        pass\n",
    "        # return pd.DataFrame({\n",
    "        #             'SerialNumber': ['Not Found'],\n",
    "        #             'AlarmTotal': ['Not Found'],\n",
    "                    \n",
    "        #             'AlarmEvent': ['Not Found'],\n",
    "        #             'AlarmId': ['Not Found'],\n",
    "        #             'AlarmMsg': ['Not Found'],\n",
    "        #             'AlarmTime': ['Not Found'],\n",
    "        #             'Channel': ['Not Found'],\n",
    "                    \n",
    "        #             'PicInfo.ObjName': ['Not Found'],\n",
    "        #             'PicInfo.ObjSize': ['Not Found'],\n",
    "        #             'PicInfo.StorageBucket': ['Not Found'],\n",
    "        #             'VideoInfo.VideoLength': ['Not Found'],\n",
    "        #             })\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_response():\n",
    "    \n",
    "    # List to store DataFrames\n",
    "    response_event_dfs = []\n",
    "    # Iterate over each log string and extract args\n",
    "    for log in event_data_log_string:\n",
    "        # Convert log string from list format to string if necessary\n",
    "        if isinstance(log, list):\n",
    "            log = log[0]  # Assuming the first element is the log string\n",
    "\n",
    "        response_event_df = get_response_event(log)\n",
    "        response_event_dfs.append(response_event_df) # string error. \n",
    "\n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    final_response_event_df = pd.concat(response_event_dfs, ignore_index=True)\n",
    "\n",
    "    # Display the final DataFrame\n",
    "    print(final_response_event_df)\n",
    "    return(final_response_event_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      AlarmEvent       AlarmId AlarmMsg            AlarmTime  \\\n",
      "0     appEventHumanDetectAlarm:2  240808153912           2024-08-08 15:39:13   \n",
      "1     appEventHumanDetectAlarm:2  240808153536           2024-08-08 15:35:37   \n",
      "2     appEventHumanDetectAlarm:2  240808153245           2024-08-08 15:32:47   \n",
      "3     appEventHumanDetectAlarm:2    2488153127           2024-08-08 15:31:35   \n",
      "4                  VideoMotion:2  240808153021           2024-08-08 15:30:21   \n",
      "...                          ...           ...      ...                  ...   \n",
      "2304               VideoMotion:2  240818170651           2024-08-18 17:06:52   \n",
      "2305               VideoMotion:2  240818162928           2024-08-18 16:29:53   \n",
      "2306               VideoMotion:2  240818162928           2024-08-18 16:29:28   \n",
      "2307               VideoMotion:2  240818155136           2024-08-18 15:51:37   \n",
      "2308               VideoMotion:2  240818144123           2024-08-18 14:41:24   \n",
      "\n",
      "     Channel                             PicInfo.ObjName  PicInfo.ObjSize  \\\n",
      "0          0  003/003_fb1f6e5c62b71703_240808153912.jpeg          26189.0   \n",
      "1          0  003/003_fb1f6e5c62b71703_240808153536.jpeg          33466.0   \n",
      "2          0  003/003_fb1f6e5c62b71703_240808153245.jpeg          34300.0   \n",
      "3          0    003/003_ba39b3962a4bfe81_2488153127.jpeg          46377.0   \n",
      "4          0  003/003_1ce32155e73d3804_240808153021.jpeg          87793.0   \n",
      "...      ...                                         ...              ...   \n",
      "2304       0  003/003_fb1f6e5c62b71703_240818170651.jpeg          35466.0   \n",
      "2305       0                                         NaN              NaN   \n",
      "2306       0  003/003_fb1f6e5c62b71703_240818162928.jpeg          35501.0   \n",
      "2307       0  003/003_fb1f6e5c62b71703_240818155136.jpeg          29545.0   \n",
      "2308       0  003/003_fb1f6e5c62b71703_240818144123.jpeg          23908.0   \n",
      "\n",
      "     PicInfo.StorageBucket  VideoInfo.VideoLength   PicErr  \n",
      "0            OBS_sg-ai-img                    NaN      NaN  \n",
      "1            OBS_sg-ai-img                    NaN      NaN  \n",
      "2            OBS_sg-ai-img                    NaN      NaN  \n",
      "3            OBS_sg-ai-img                    NaN      NaN  \n",
      "4            VTT_camera151                    NaN      NaN  \n",
      "...                    ...                    ...      ...  \n",
      "2304         OBS_sg-ai-img                    NaN      NaN  \n",
      "2305                   NaN                    NaN -30007.0  \n",
      "2306         OBS_sg-ai-img                   10.0      NaN  \n",
      "2307         OBS_sg-ai-img                    NaN      NaN  \n",
      "2308         OBS_sg-ai-img                    NaN      NaN  \n",
      "\n",
      "[2309 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "final_response_event_df = fetch_response()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_response_event_df.to_csv('./csv_output/alarm_response_v3.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
